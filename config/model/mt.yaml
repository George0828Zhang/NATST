# @package _group_

_name: mt_transformer
input_feat_per_channel: -1
input_channels: -1

encoder_embed_dim: 256
encoder_ffn_embed_dim: 2048
encoder_layers: 6
encoder_attention_heads: 4
encoder_normalize_before: true  

decoder_embed_dim: ${model.encoder_embed_dim}
decoder_ffn_embed_dim: ${model.encoder_ffn_embed_dim}
decoder_layers: 6
decoder_attention_heads: 4
decoder_normalize_before: true
decoder_output_dim: ${model.encoder_embed_dim}
decoder_input_dim: ${model.encoder_embed_dim} 

share_all_embeddings: true

dropout: 0.3
attention_dropout: 0.1
activation_dropout: 0.1
activation_fn: "gelu"